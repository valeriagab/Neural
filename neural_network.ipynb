{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,derivative=False):\n",
    "    \"\"\"\n",
    "    This is the activation function. It takes as input\n",
    "    z = Wx + b, and return the sigmoid function or\n",
    "    the derivation of the sigmoid function\n",
    "    \"\"\"\n",
    "    \n",
    "    if derivative :\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-x))\n",
    "def cost_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Here we compute the loss function between two predictions.\n",
    "    \"\"\"\n",
    "    n = y_pred.shape[0]\n",
    "    cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n",
    "    return cost\n",
    "\n",
    "        \n",
    "    \n",
    "class Net():\n",
    "    \"\"\"\n",
    "    Building The Network model with weight and bias \n",
    "    Each row is a node, each column is a dimension of the input,\n",
    "    so dimension is (n_nodes, input_dim).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,size,seed=42):\n",
    "        self.seed=seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size=size\n",
    "        self.weights=[np.random.randn(size[i-1],size[i])for i in range(1,len(size))]\n",
    "        self.biases=[np.random.randn(1,size[i]) for i in range(1,len(size))]\n",
    "        \n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        This process the forward propagation step\n",
    "        \"\"\"\n",
    "        activations=[input]\n",
    "        pre_activations=[]\n",
    "        for weight,bias in zip(self.weights,self.biases):\n",
    "            pre_activations.append(np.dot(activations[-1],weight)+bias)\n",
    "            activations.append(sigmoid(pre_activations[-1]))\n",
    "        return pre_activations,activations\n",
    "    \n",
    "    \n",
    "    def cost_derivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        loss function derivatives\n",
    "        \"\"\"\n",
    "        \n",
    "        return (y_pred-y_true)\n",
    "    \n",
    "    \n",
    "    def train(self,x,y,learning_rate,epoch):\n",
    "        \"\"\"\n",
    "        This process the backward propagation algorithm.\n",
    "        \"\"\"\n",
    "        for e in range(int(epoch)):\n",
    "            \n",
    "            nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "            nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            z,a =self.forward(x)\n",
    "            n = y.shape[0]\n",
    "            error = self.cost_derivative(a[-1],y)\n",
    "            delta = error*sigmoid(z[-1],derivative=True)/n\n",
    "            # Update last  layer\n",
    "            nabla_b[-1] = np.mean(delta,axis=0)\n",
    "            nabla_w[-1] = np.dot(a[-2].T,delta)\n",
    "     \n",
    "            for i in range(1,len(self.size)-1):              \n",
    "                # Update all other layers\n",
    "                delta=np.dot(delta,self.weights[-i].T)*sigmoid(z[-i-1],derivative=True)/n\n",
    "                nabla_b[-i-1] = np.mean(delta,axis=0)\n",
    "                nabla_w[-i-1] = np.dot( a[-i-2].T,delta)\n",
    "            self.weights = [w-learning_rate*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b-learning_rate*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "            if e % 10000 ==0:\n",
    "                # Display loss\n",
    "                print(cost_function(a[-1],y))\n",
    "                \n",
    "                \n",
    "    def predict(self, a):\n",
    "        activation=self.forward(a)[1]\n",
    "        return activation[-1]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[[\"SepalLengthCm\",\"SepalWidthCm\",\"PetalLengthCm\",\"PetalWidthCm\"]].values\n",
    "y=data[[\"Species\"]].replace(['Iris-setosa','Iris-versicolor','Iris-virginica'],[0,1,2]).values\n",
    "target=[[0,0,1],[0,1,0],[1,0,0]]\n",
    "y=np.array([target[int(x)] for x in y ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26802499509629607\n",
      "0.0949293667152557\n",
      "0.058300369614221824\n",
      "0.044028288099383514\n",
      "0.03750861489276395\n"
     ]
    }
   ],
   "source": [
    "nn=Net([4,5,3])\n",
    "nn.train(X_train,y_train,0.1,50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check our model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=nn.predict(X_test)\n",
    "accuracy_score(np.argmax(y_pred, axis=1),np.argmax(y_test,axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
