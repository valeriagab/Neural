{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neuralstepik.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9hmeCK7gB6U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhx0Xmp7gF4F",
        "colab_type": "code",
        "outputId": "cb3da664-c567-455b-8fde-aa9cd4563a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import sklearn.datasets\n",
        "wine = sklearn.datasets.load_wine()\n",
        "wine.data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(178, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kletBoThgJH1",
        "colab_type": "code",
        "outputId": "4782ad2b-2491-497c-b8e6-61f3e3008c67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "pd.DataFrame(data=wine.data,columns=wine.feature_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alcohol</th>\n",
              "      <th>malic_acid</th>\n",
              "      <th>ash</th>\n",
              "      <th>alcalinity_of_ash</th>\n",
              "      <th>magnesium</th>\n",
              "      <th>total_phenols</th>\n",
              "      <th>flavanoids</th>\n",
              "      <th>nonflavanoid_phenols</th>\n",
              "      <th>proanthocyanins</th>\n",
              "      <th>color_intensity</th>\n",
              "      <th>hue</th>\n",
              "      <th>od280/od315_of_diluted_wines</th>\n",
              "      <th>proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113.0</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95.0</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96.0</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows Ã— 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     alcohol  malic_acid   ash  ...   hue  od280/od315_of_diluted_wines  proline\n",
              "0      14.23        1.71  2.43  ...  1.04                          3.92   1065.0\n",
              "1      13.20        1.78  2.14  ...  1.05                          3.40   1050.0\n",
              "2      13.16        2.36  2.67  ...  1.03                          3.17   1185.0\n",
              "3      14.37        1.95  2.50  ...  0.86                          3.45   1480.0\n",
              "4      13.24        2.59  2.87  ...  1.04                          2.93    735.0\n",
              "..       ...         ...   ...  ...   ...                           ...      ...\n",
              "173    13.71        5.65  2.45  ...  0.64                          1.74    740.0\n",
              "174    13.40        3.91  2.48  ...  0.70                          1.56    750.0\n",
              "175    13.27        4.28  2.26  ...  0.59                          1.56    835.0\n",
              "176    13.17        2.59  2.37  ...  0.60                          1.62    840.0\n",
              "177    14.13        4.10  2.74  ...  0.61                          1.60    560.0\n",
              "\n",
              "[178 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFs6OFs0gNa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data[:, :],  \n",
        "    wine.target, \n",
        "    test_size=0.3, \n",
        "    shuffle=True)\n",
        "\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1oFEoBdg5er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WineNet(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_neurons):\n",
        "        super(WineNet, self).__init__()\n",
        "        \n",
        "        self.fc1 = torch.nn.Linear(13, n_hidden_neurons)\n",
        "        self.activ1 = torch.nn.Sigmoid()\n",
        "        self.fc2 = torch.nn.Linear(n_hidden_neurons, n_hidden_neurons)\n",
        "        self.activ2 = torch.nn.Sigmoid()\n",
        "        self.fc3 = torch.nn.Linear(n_hidden_neurons, 3)\n",
        "        self.sm = torch.nn.Softmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activ1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activ2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def inference(self, x):\n",
        "        x = self.forward(x)\n",
        "        x = self.sm(x)\n",
        "        return x\n",
        "    \n",
        "wine_net = WineNet(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7qryXSNhAT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(wine_net.parameters(), \n",
        "                             lr=1.0e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td-9loFBhDYx",
        "colab_type": "code",
        "outputId": "993427ad-9b4c-4581-bbbe-56e0becd87e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "batch_size = 80\n",
        "\n",
        "for epoch in range(2000):\n",
        "    order = np.random.permutation(len(X_train))\n",
        "    for start_index in range(0, len(X_train), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        batch_indexes = order[start_index:start_index+batch_size]\n",
        "        \n",
        "        x_batch = X_train[batch_indexes]\n",
        "        y_batch = y_train[batch_indexes]\n",
        "        \n",
        "        preds = wine_net.forward(x_batch) \n",
        "        \n",
        "        loss_value = loss(preds, y_batch)\n",
        "        loss_value.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        test_preds = wine_net.forward(X_test)\n",
        "        test_preds = test_preds.argmax(dim=1)\n",
        "        print((test_preds == y_test).float().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7222)\n",
            "tensor(0.7037)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.7037)\n",
            "tensor(0.6852)\n",
            "tensor(0.7037)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.7037)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n",
            "tensor(0.6852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Cl3RippXF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}